# TIF-Contrast: Similarity Detection for Academic Documents via Triplet-aligned Text-Image-Formula Embeddings
This is the official implementation of the paper: "Similarity Detection for Academic Documents via Triplet-aligned Text-Image-Formula Embeddings and Contrastive Learning", currently submitted to Information Sciences.

## Abstract
Academic similarity detection is fundamental to research integrity. However, effective detection in multimodal documents is hindered by the semantic heterogeneity across text, figures, and mathematical expressions. We propose the Triplet-aligned Text-Image-Formula Contrastive Similarity Framework (TIF-Contrast).

Central to our approach is a triplet-aligned embedding mechanism that projects diverse modalities into a unified semantic space. By constructing contrastive "anchor-positive-negative" modality triplets, the framework forces semantically consistent cross-modal content to cluster closely in the shared space, enabling accurate, end-to-end similarity retrieval for papers, theses, and technical documents.

## Key Contributions
- Triplet-Aligned Embeddings: A novel strategy to constrain feature distribution across text, image, and formula modalities, bridging the gap of semantic heterogeneity.
- Unified Semantic Space: Facilitates direct cross-modal retrieval without the need for complex, manual late-fusion heuristics.
- Adaptive Multimodal Fusion: Specifically designed to handle variable modal distributions (e.g., documents with varying densities of formulas or figures).
- End-to-End Framework: Optimized for large-scale academic document databases to identify sophisticated plagiarism.

## Framework Architecture
TIF-Contrast utilizes three specialized encoders feeding into a shared contrastive learning objective:
- Text Encoder: Captures the semantic nuances of academic language.
- Image Encoder: Extracts visual features from figures, charts, and tables.
- Formula Encoder: A structural encoder designed for the symbolic semantics of mathematical expressions (LaTeX/MathML).

The alignment is governed by a Triplet Margin Loss:
$$L(a, p, n) = \max(0, d(f(a), f(p)) - d(f(a), f(n)) + \text{margin})$$
where $a, p, n$ represent anchor, positive, and negative modality samples respectively.

## Getting Started
### Prerequisites
- Python 3.9+
- PyTorch 2.0+
- CUDA 11.8+ (Recommended)

### Installation
```bash
git clone https://github.com/YourUsername/TIF-Contrast.git
cd TIF-Contrast
pip install -r requirements.txt
# What to align in multimodal contrastive learning ? 

[Benoit Dufumier*](https://scholar.google.fr/citations?user=r64E9-IAAAAJ) & [Javiera Castillo Navarro*](https://javi-castillo.github.io), [Devis Tuia](https://people.epfl.ch/devis.tuia), [Jean-Philippe Thiran](https://people.epfl.ch/jean-philippe.thiran)

[![paper](https://img.shields.io/badge/arXiv-Paper-<COLOR>.svg)](https://arxiv.org/abs/2409.07402)
[![website](https://img.shields.io/badge/Website-Site-blue)](https://javi-castillo.github.io/comm)
[![Notebook Demo](https://img.shields.io/badge/Jupyter-Demo-orange?logo=jupyter)](https://github.com/Duplums/align_or_not/blob/main/demo/trifeatures.ipynb)
[![BibTeX](https://img.shields.io/badge/BibTeX-Cite-blueviolet.svg)](#citation)

## Overview 

**Abstract**: Humans perceive the world through multisensory integration, blending the information of different modalities to adapt their behavior. Alignment through contrastive learning offers an appealing solution for multimodal self-supervised learning. Indeed, by considering each modality as a different view of the same entity, it learns to align features of different modalities in a shared representation space. However, this approach is intrinsically limited as it only learns shared or redundant information between modalities, while multimodal interactions can arise in other ways.
In this work, we introduce *CoMM*, a **Co**ntrastive **M**ulti**M**odal learning strategy that enables the **comm**unication between modalities in a single multimodal space. Instead of imposing cross- or intra- modality constraints, we propose to align multimodal representations by maximizing the mutual information between augmented versions of these multimodal features. Our theoretical analysis shows that shared, synergistic and unique terms of information naturally emerge from this formulation, allowing to estimate multimodal interactions beyond redundancy. We test CoMM both in a controlled and in a series of real-world settings: in the former, we demonstrate that CoMM effectively captures redundant, unique and synergistic information between modalities. In the latter, we show that CoMM learns complex multimodal interactions and achieves state-of-the-art results on seven multimodal tasks.

<div align="center">
  <span style="display: inline-block; background: white; padding: 10px; border-radius: 5px;">
  <img src="assets/CoMM_training.png" width="60%" />
  </span>
</div>

## Jupyter notebooks to reproduce our experiments üöÄ

This repository contains different Jupyter notebooks to reproduce the experiments we performed in our paper. They are self-contained.

| Notebook | Description |
|----------|------------|
| [trifeatures.ipynb](demo/trifeatures.ipynb) | Controlled experiments on synthetic bimodal Trifeatures |
| [multibench.ipynb](demo/multibench.ipynb) | Real-world MultiBench experiments |
| [multibench_mimic.ipynb](demo/multibench_mimic.ipynb) | MIMIC experiments (require credentials) |
| [mmimdb.ipynb](demo/mmimdb.ipynb) | Text-Image MM-IMDb experiments  |
| [trimodal.ipynb](demo/trimodal.ipynb) | Vision&Touch experiments with 3 modalities |

---

## How to run the model locally ?

### Installation

You can install all the packages required to run CoMM with conda:

```sh
git clone https://github.com/Duplums/CoMM && cd CoMM
conda env create -f environment.yml
conda activate multimodal
```

### Controlled experiments on synthetic bimodal dataset

We first evaluate our proposed method CoMM against FactorCL, Cross and Cross+Self models on the synthetic bimodal Trifeatures dataset. It consists of pairs of images with controllable texture, shape and color. These pairs share shape as common features but with distinct colors and textures. Please check [this notebook](demo/trifeatures.ipynb) for more details.

#### Shell script

```sh
python3 main_trifeatures.py \
      +data=trifeatures \
      +data.data_module.biased=false \ # Set 'true' only for experiments on synergy
      +model=comm \
      model.model.encoder.embed_dim=512 \
      mode="train" \
      trainer.max_epochs=100
```

#### Results

<p float="left">
  <img src="assets/CoMM_results_trifeatures.png" width="50%" />
</p>


## Experiments on MultiBench

Then, we perform experiments on real-world MultiBench datasets including videos (audio, image, speech), tabular data (medical recordings), medical timeseries from ICU, force and proprioception data from robotics. Check [this notebook](demo/multibench.ipynb) for a demo.

#### Shell script

```sh
dataset="mosi" # Can be in ["mosi", "humor", "sarcasm", "mimic", "visionandtouch", "visionandtouch-bin"]
python3 main_multibench.py \
      data.data_module.dataset=${dataset} \
      model="comm" \
      trainer.max_epochs=100 \
      optim.lr=1e-3 \
      optim.weight_decay=1e-2
```

#### Results

Linear evaluation top-1 accuracy (in %, averaged over 5 runs) for classification tasks and MSE (√ó10‚Åª‚Å¥) for regression task (V&T) on MultiBench after 100 epochs. 

---
| Model                 | V&T Reg‚Üì  | MIMIC‚Üë  | MOSI‚Üë  | UR-FUNNY‚Üë  | MUsTARD‚Üë  | Average‚Üë  |
|:--------------------:|:---------------------:|:--------:|:-----:|:----------:|:---------:|:-------------:|
| Cross                | 33.09                 | 66.7    | 47.8   | 50.1       | 53.5      | 54.52         |
| Cross+Self           | 7.56                  | 65.49   | 49.0   | 59.9       | 53.9      | 57.07         |
| FactorCL             | 10.82                 | **67.3** | 51.2   | 60.5       | 55.80     | 58.7         |
| **CoMM (ours)**      | **4.55**              | 66.4    | **67.5** | **63.1**  | **63.9**  | **65.22**    |
|                      |                       |         |        |            |           |               |
| üü£SupCon        | -                     | 67.4    | 47.2   | 50.1       | 52.7      | 54.35          |
| üü£FactorCL-SUP  | 1.72                  | **76.8** | 69.1   | 63.5       | 69.9      | 69.82         |
| üü£**CoMM** | **1.34**          | 68.18   | **74.98** | **65.96** | **70.42** | **69.88**     |
---

Rows in üü£ means supervised fine-tuning. Average is taken over classification results only.


## Experiments on MM-IMDb

Next, we focus on MM-IMDb, a large multimodal dataset for movie genre prediction, including image (movie poster) and text (plot) pairs. Each movie can be classified into one or more genres, thus the downstream task is multi-class multi-label with 23 categories. You can check [this notebook](demo/mmimdb.ipynb) for a demo. 

#### Shell script

```sh
python3 main_mmimdb.py \
      +model=comm \
      model.model.encoder.embed_dim=768 \
      +data=mmimdb \
      mode="train" \
      +mmimdb.encoders.1.mask_prob=0.15 \
      trainer.max_epochs=100 
```

#### Results

Linear evaluation for multi-class multi-label classification prediction of movie genres (in %, averaged over 5 runs) after 70 epochs.

---
| Model                                    | Modalities | Weighted-F1‚Üë  | Macro-F1‚Üë  |
|:----------------------------------------:|:----------:|:-------------:|:-----------:|
| SimCLR                                   | V          | 40.35         | 27.99       |
| CLIP                                     | V          | 51.5          | 40.8        |
| CLIP                                     | L          | 51.0          | 43.0        |
| CLIP                                     | V+L        | 58.9          | 50.9        |
| BLIP-2                                   | V+L        | 57.4          | 49.9        |
| SLIP                                     | V+L        | 56.54         | 47.35       |
| **CoMM (w/CLIP)**           | V+L        | _61.48_       | _54.63_     |
| **CoMM (w/BLIP-2)**         | V+L        | **64.75**     | **58.44**   |
|                                          |            |               |             |
| üü£ **MFAS**                              | V+L        | 62.50         | 55.6        |
| üü£ **ReFNet**                            | V+L        | -             | 56.7        |
| üü£ **CoMM (w/CLIP)**  | V+L              | _64.90_       | _58.97_     |
| üü£ **CoMM (w/BLIP-2)**| V+L              | **67.39**     | **62.0**    |
|                                          |            |               |             |
| LLaVA-NeXT                               | V+L        | 64.28         | 56.51       |
---
Rows in üü£ means supervised fine-tuning.


## Experiments with 3 modalities

Finally, we perform CoMM on multimodal datasets with more than 3 modalities. We focus here on Vision&Touch, a robotic dataset with visual, force-torque and proprioception modalities when performing a peg insertion task. 150 trajectories are recorded, each of them consisting in 1000 timesteps. Check [this notebook](demo/trimodal.ipynb) for a demo.

#### Shell script
```sh
python3 main_multibench_all-mod.py \
      +model=comm \
      model.model.encoder.embed_dim=512 \
      +data=multibench \
      data.data_module.dataset="visionandtouch-bin" \
      mode="train" \
      trainer.max_epochs=100 
```

#### Results

Linear evaluation top-1 accuracy (%) on Vision&Touch and UR-FUNNY.

| Model                        | #Mod. | V&T CP | UR-FUNNY |
|------------------------------|-------|--------|----------|
| Cross                        | 2     | 84.4   | 50.1     |
| Cross+Self                   | 2     | 86.8   | 59.9     |
| CoMM *(ours)*                | 2     | *88.1* | *63.1*   |
| CMC                          | 3     | 94.1   | 59.2     |
| CoMM *(ours)*                | 3     | **94.2** | **64.6** |


## Citation

If you use CoMM, you may cite:

    @inproceedings{dufumier_castillo2025, 
        title={What to align in multimodal contrastive learning?},
        author={Dufumier, Benoit and Castillo-Navarro, Javiera and Tuia, Devis and Thiran, Jean-Philippe},
        booktitle={International Conference on Learning Representations},
        year={2025}
    }
